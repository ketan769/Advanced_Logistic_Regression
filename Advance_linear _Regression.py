# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hkKJtwEQ7kUxUwPb5sz4KE8zVMTqkvuH
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder


data_train=pd.read_csv('train.csv')
data_test=pd.read_csv('test.csv')


data_label=data_train.SalePrice
data_train.drop('SalePrice',axis=1,inplace=True)

data_comb=pd.concat([data_train,data_test],axis=0)

data_comb=data_comb.drop(['PoolQC',
       'Fence', 'MiscFeature','FireplaceQu','Alley'],axis=1)

for i in np.unique(data_comb['Neighborhood'].values):
  print(i)
  data_comb['LotFrontage'][data_comb['Neighborhood']==i]=data_comb['LotFrontage'][data_comb['Neighborhood']==i].fillna(data_comb['LotFrontage'][data_comb['Neighborhood']==i].mean())

for j in data_check_categorical.columns:
    
    data_comb[j]=data_comb[j].fillna(data_comb[j].mode()[0])
data_comb['GarageYrBlt']=data_comb['GarageYrBlt'].fillna(data_comb['GarageYrBlt'].mode()[0])
col=['BsmtFinSF1','BsmtFinSF2','TotalBsmtSF','BsmtUnfSF','BsmtFullBath','BsmtHalfBath','GarageYrBlt','GarageCars','GarageArea','MasVnrArea']
for i in col:
  data_comb[i]=data_comb[i].fillna(data_comb[i].mean())

data_comb.reset_index(inplace=True)

for j in data_check_categorical.columns:
  temp=data_comb[j]
  temp=temp[:,np.newaxis]
  encode=OneHotEncoder()
  encode.fit(temp)
  temp=encode.transform(temp).toarray()
  list1=[j[:8]+'{}'.format(i) for i in range(1,temp.shape[1]+1)]
  temp=pd.DataFrame(temp,columns=list1)
  data_comb=pd.concat([data_comb,temp],axis=1)
  data_comb=data_comb.drop([j],axis=1)
  
  
from sklearn.model_selection import train_test_split
x_train,x_val,y_train,y_val=train_test_split(data_train1,data_label,test_size=0.3,random_state=115)


model_LR=LinearRegression()
model_LR.fit(x_train,y_train)
score=model_LR.score(x_train,y_train)
cross_val_score(model_LR,x_train,y_train,cv=3)
score=model_LR.score(x_val,y_val)

model_Lasso=Lasso(alpha=1)

model_Lasso.fit(x_train,y_train)
score=model_Lasso.score(x_train,y_train)
score_val=model_Lasso.score(x_val,y_val)
model_RF=RandomForestRegressor(n_estimators=40)
model_RF.fit(x_train,y_train)
model_RF.score(x_train,y_train)
model_RF.score(x_val,y_val)
cross_val_score(model_RF,x_train,y_train,cv=3)
importances = model_RF.feature_importances_
indices = np.argsort(importances)

plt.figure(1,figsize=(50,50))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), x_train.columns[indices])
plt.xlabel('Relative Importance')


model_Gb=GradientBoostingRegressor(learning_rate=0.1,n_estimators=150)
model_Gb.fit(x_train,y_train)
model_Gb.score(x_train,y_train)



from sklearn.ensemble import VotingRegressor
ensemble_model=VotingRegressor(estimators=[('gb',model_Gb),('Rf',model_RF),('LR',model_Lasso)],weights=[0.6,0.2,0.2])
ensemble_model.fit(x_train1,y_train)
ensemble_model.score(x_val1,y_val)
ensemble_model.fit(np.array(data_train1),data_label)
ensemble_model.score(np.array(data_train1),data_label)
y_pred=ensemble_model.predict(np.array(data_test1))